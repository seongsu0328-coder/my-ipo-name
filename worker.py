import os
import time
import json
import re
import requests
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta, date
import pytz 
from supabase import create_client
import google.generativeai as genai

# ==========================================
# [1] í™˜ê²½ ì„¤ì •
# ==========================================
SUPABASE_URL = os.environ.get("SUPABASE_URL", "").rstrip('/')
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")
GENAI_API_KEY = os.environ.get("GENAI_API_KEY", "")
FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY", "")

if not (SUPABASE_URL and SUPABASE_KEY):
    print("âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½")
    exit()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# AI ëª¨ë¸ ì„¤ì •
model = None 
if GENAI_API_KEY:
    genai.configure(api_key=GENAI_API_KEY)
    try:
        model = genai.GenerativeModel('gemini-2.0-flash', tools=[{'google_search_retrieval': {}}])
        print("âœ… AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    except:
        model = genai.GenerativeModel('gemini-2.0-flash')

# ==========================================
# [2] í—¬í¼ í•¨ìˆ˜: ì™„ë²½í•œ ë°ì´í„° ì •ì œ ë° ì§ì†¡
# ==========================================

def sanitize_value(v):
    if v is None or pd.isna(v): return None
    if isinstance(v, (np.floating, float)):
        return float(v) if not (np.isinf(v) or np.isnan(v)) else 0.0
    if isinstance(v, (np.integer, int)): return int(v)
    if isinstance(v, (np.bool_, bool)): return bool(v)
    return str(v).strip().replace('\x00', '')

def batch_upsert(table_name, data_list, on_conflict="ticker"):
    """405 ì—ëŸ¬ë¥¼ ì›ì²œ ì°¨ë‹¨í•˜ëŠ” ë²”ìš© REST API Upsert"""
    if not data_list: return
    
    # URL ê²½ë¡œ ìë™ êµì • ë¡œì§
    base_url = SUPABASE_URL
    if "/rest/v1" not in base_url:
        endpoint = f"{base_url}/rest/v1/{table_name}?on_conflict={on_conflict}"
    else:
        endpoint = f"{base_url}/{table_name}?on_conflict={on_conflict}"
    
    headers = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "resolution=merge-duplicates"
    }

    print(f"ğŸš€ [{table_name}] {len(data_list)}ê°œ ì‹œë„ (ê¸°ì¤€: {on_conflict})")
    success_save = 0
    
    for item in data_list:
        clean_payload = {k: sanitize_value(v) for k, v in item.items()}
        if not clean_payload.get(on_conflict): continue

        try:
            # ê°œë³„ ì „ì†¡ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            resp = requests.post(endpoint, json=clean_payload, headers=headers)
            if resp.status_code in [200, 201, 204]:
                success_save += 1
            else:
                print(f"   âš ï¸ {clean_payload.get(on_conflict)} ì‹¤íŒ¨ ({resp.status_code}): {resp.text[:100]}")
        except: continue
    print(f"ğŸ [{table_name}] ì„±ê³µ: {success_save}")

# ==========================================
# [3] ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ ìœ ì§€)
# ==========================================

def get_target_stocks():
    if not FINNHUB_API_KEY: return pd.DataFrame()
    now = datetime.now()
    all_data = []
    # ë°ì´í„° ë²”ìœ„ë¥¼ ìŠ¹ìˆ˜ë‹˜ ìš”ì²­ëŒ€ë¡œ ë„“ê²Œ ì„¤ì •
    ranges = [(now-timedelta(days=200), now+timedelta(days=35)), (now-timedelta(days=560), now-timedelta(days=350))]
    for start_dt, end_dt in ranges:
        url = f"https://finnhub.io/api/v1/calendar/ipo?from={start_dt.strftime('%Y-%m-%d')}&to={end_dt.strftime('%Y-%m-%d')}&token={FINNHUB_API_KEY}"
        try:
            res = requests.get(url, timeout=10).json()
            if res.get('ipoCalendar'): all_data.extend(res['ipoCalendar'])
        except: continue
    if not all_data: return pd.DataFrame()
    df = pd.DataFrame(all_data).dropna(subset=['symbol'])
    return df.drop_duplicates(subset=['symbol'])

def update_all_prices_batch(df_target):
    print("\nğŸ’° [ì •ë°€ ìƒíƒœ ë¶„ì„] ì‹œì‘...")
    upsert_list = []
    now_iso = datetime.now().isoformat()
    for t in df_target['symbol'].tolist():
        try:
            stock = yf.Ticker(t)
            hist = stock.history(period="1d")
            status = "Active" if not hist.empty else ("ìƒì¥ì—°ê¸°" if stock.info.get('symbol') else "ìƒì¥íì§€")
            price = float(round(hist['Close'].iloc[-1], 4)) if not hist.empty else 0.0
            upsert_list.append({"ticker": t, "price": price, "status": status, "updated_at": now_iso})
        except:
            upsert_list.append({"ticker": t, "price": 0.0, "status": "ìƒì¥íì§€", "updated_at": now_iso})
    batch_upsert("price_cache", upsert_list, on_conflict="ticker")

# ==========================================
# [4] AI ë¶„ì„ (í”„ë¡¬í”„íŠ¸ 100% ë³µì›)
# ==========================================

def run_tab0_analysis(ticker, company_name):
    if not model: return
    for topic in ["S-1", "424B4"]:
        points = "Risk Factors, MD&A" if topic == "S-1" else "Final Price, Underwriting"
        prompt = f"ë‹¹ì‹ ì€ ì›”ê°€ ë¶„ì„ê°€ì…ë‹ˆë‹¤. {company_name}({ticker})ì˜ {topic} ì„œë¥˜ë¥¼ ë¶„ì„í•˜ì„¸ìš”. {points}ë¥¼ í¬í•¨í•˜ì—¬ í•œêµ­ì–´ë¡œ 3ë¬¸ì¥ì”© ì‘ì„±í•˜ì„¸ìš”."
        try:
            resp = model.generate_content(prompt)
            batch_upsert("analysis_cache", [{"cache_key": f"{company_name}_{topic}_Tab0", "content": resp.text, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
        except: pass

def run_tab1_analysis(ticker, company_name):
    if not model: return
    now_str = datetime.now().strftime("%Y-%m-%d")
    prompt = f"""ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. {company_name}({ticker}) ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    1. í•œêµ­ì–´ë§Œ ì‚¬ìš© 2. 3ê°œ ë¬¸ë‹¨ êµ¬ì„±(ë¹„ì¦ˆë‹ˆìŠ¤, ì¬ë¬´, ì „ë§) 3. ì¸ì‚¬ë§ ì ˆëŒ€ ê¸ˆì§€.
    ë§ˆì§€ë§‰ì— <JSON_START> {{"news": []}} <JSON_END> í˜•íƒœë¡œ ë‰´ìŠ¤ 5ê°œë¥¼ í¬í•¨í•˜ì„¸ìš”."""
    try:
        resp = model.generate_content(prompt)
        full_text = resp.text
        biz_analysis = full_text.split("<JSON_START>")[0].strip()
        paragraphs = [p.strip() for p in biz_analysis.split('\n') if len(p.strip()) > 20]
        html = "".join([f'<p style="margin-bottom:15px; line-height:1.7;">{p}</p>' for p in paragraphs])
        
        news = []
        if "<JSON_START>" in full_text:
            try: news = json.loads(full_text.split("<JSON_START>")[1].split("<JSON_END>")[0])["news"]
            except: pass
        
        batch_upsert("analysis_cache", [{"cache_key": f"{ticker}_Tab1", "content": json.dumps({"html": html, "news": news}, ensure_ascii=False), "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
    except: pass

# (Tab 2, 3, 4 ìƒëµí•˜ì§€ë§Œ ë¡œì§ì€ ìœ„ì™€ ë™ì¼í•˜ê²Œ on_conflict="cache_key" ì ìš©)
# (Tab 0) ì£¼ìš” ê³µì‹œ ë¶„ì„
def run_tab0_analysis(ticker, company_name):
    if not model: return
    for topic in ["S-1", "424B4"]:
        cache_key = f"{company_name}_{topic}_Tab0"
        
        if topic == "S-1":
            points = "Risk Factors, Use of Proceeds, MD&A"
            structure = """
            1. **[íˆ¬ìí¬ì¸íŠ¸]** : í•´ë‹¹ ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ê°€ì¥ ì¤‘ìš”í•œ íˆ¬ì í¬ì¸íŠ¸ë¥¼ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê·¼ê±°ì™€ í•¨ê»˜ ìƒì„¸íˆ ì„œìˆ í•˜ì„¸ìš”.
            2. **[ì„±ì¥ê°€ëŠ¥ì„±]** : MD&A(ê²½ì˜ì§„ ë¶„ì„)ë¥¼ í†µí•´ ë³¸ ê¸°ì—…ì˜ ì‹¤ì§ˆì  ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ ì¬ë¬´ì  í•¨ì˜ë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì„¸ìš”.
            3. **[í•µì‹¬ë¦¬ìŠ¤í¬]** : íˆ¬ììê°€ ë°˜ë“œì‹œ ê²½ê³„í•´ì•¼ í•  í•µì‹¬ ë¦¬ìŠ¤í¬ 1ê°€ì§€ì™€ ê·¸ íŒŒê¸‰ íš¨ê³¼ ë° ëŒ€ì‘ì±…ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.
            """
        else:
            points = "Final Price, Use of Proceeds, Underwriting"
            structure = """
            1. **[ìµœì¢…ê³µëª¨ê°€]** : í™•ì •ëœ ê³µëª¨ê°€ê°€ í¬ë§ ë°´ë“œ ìƒë‹¨ì¸ì§€ í•˜ë‹¨ì¸ì§€ ë¶„ì„í•˜ê³ , ê·¸ ì˜ë¯¸(ì‹œì¥ ìˆ˜ìš”)ë¥¼ í•´ì„í•˜ì„¸ìš”.
            2. **[ìê¸ˆí™œìš©]** : í™•ì •ëœ ì¡°ë‹¬ ìê¸ˆì´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ìš°ì„ ìˆœìœ„ ì‚¬ì—…ì— íˆ¬ì…ë  ì˜ˆì •ì¸ì§€ ìµœì¢… ì ê²€í•˜ì„¸ìš”.
            3. **[ìƒì¥í›„ ì „ë§]** : ì£¼ê´€ì‚¬ë‹¨ êµ¬ì„±ê³¼ ë°°ì • ë¬¼ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì¥ ì´ˆê¸° ìœ í†µ ë¬¼ëŸ‰ ë¶€ë‹´ì´ë‚˜ ë³€ë™ì„±ì„ ì˜ˆì¸¡í•˜ì„¸ìš”.
            """

        prompt = f"""
        ë¶„ì„ ëŒ€ìƒ: {company_name} ({ticker})ì˜ {topic} ì„œë¥˜
        ì²´í¬í¬ì¸íŠ¸: {points}
        [ì§€ì¹¨] ë‹¹ì‹ ì€ ì›”ê°€ ì¶œì‹ ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì¸ì‚¬ë§ ì—†ì´ ë°”ë¡œ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.
        [ë‚´ìš© êµ¬ì„±] {structure}
        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì–´ì¡°ì˜ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. (ê° í•­ëª©ë‹¹ 3~4ë¬¸ì¥)
        """
        try:
            response = model.generate_content(prompt)
            batch_upsert("analysis_cache", [{
                "cache_key": cache_key,
                "content": response.text,
                "updated_at": datetime.now().isoformat()
            }], on_conflict="cache_key")
        except: pass

# (Tab 1) ë¹„ì¦ˆë‹ˆìŠ¤ & ë‰´ìŠ¤ ë¶„ì„
def run_tab1_analysis(ticker, company_name):
    if not model: return False
    now = datetime.now()
    current_date = now.strftime("%Y-%m-%d")
    one_year_ago = (now - timedelta(days=365)).strftime("%Y-%m-%d")
    cache_key = f"{ticker}_Tab1"
    
    prompt = f"""
    ë‹¹ì‹ ì€ í•œêµ­ ìµœê³ ì˜ ì¦ê¶Œì‚¬ ë¦¬ì„œì¹˜ ì„¼í„°ì˜ ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    ë¶„ì„ ëŒ€ìƒ: {company_name} ({ticker})
    ì˜¤ëŠ˜ ë‚ ì§œ: {current_date}

    [ì‘ì—… 1: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì‹¬ì¸µ ë¶„ì„]
    ì•„ë˜ [í•„ìˆ˜ ì‘ì„± ì›ì¹™]ì„ ì¤€ìˆ˜í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    1. ì–¸ì–´: ì˜¤ì§ 'í•œêµ­ì–´'ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ì–´ ê³ ìœ ëª…ì‚¬ ì œì™¸). 
    2. í¬ë§·: ë°˜ë“œì‹œ 3ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì„¸ìš”. ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” ì¤„ë°”ê¿ˆì„ ëª…í™•íˆ ë„£ìœ¼ì„¸ìš”.
       - 1ë¬¸ë‹¨: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ê²½ìŸ ìš°ìœ„ (ë…ì ë ¥, ì‹œì¥ ì§€ë°°ë ¥ ë“±)
       - 2ë¬¸ë‹¨: ì¬ë¬´ í˜„í™© ë° ê³µëª¨ ìê¸ˆ í™œìš© (ë§¤ì¶œ ì¶”ì´, í‘ì ì „í™˜ ì—¬ë¶€, ìê¸ˆ ì‚¬ìš©ì²˜)
       - 3ë¬¸ë‹¨: í–¥í›„ ì „ë§ ë° íˆ¬ì ì˜ê²¬ (ì‹œì¥ ì„±ì¥ì„±, ë¦¬ìŠ¤í¬ ìš”ì¸ í¬í•¨)
    3. ë¬¸ì²´: '~ìŠµë‹ˆë‹¤' ì²´ë¥¼ ì‚¬ìš©í•˜ë˜, ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„±í•˜ì„¸ìš”.
       - [ì¤‘ìš”] ëª¨ë“  ë¬¸ì¥ì´ ê¸°ì—…ëª…(ì˜ˆ: 'ë™ì‚¬ëŠ”', '{company_name}ì€')ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
    4. ê¸ˆì§€: ì œëª©, ì†Œì œëª©, íŠ¹ìˆ˜ê¸°í˜¸, ë¶ˆë ›í¬ì¸íŠ¸(-)ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.

    [ì‘ì—… 2: ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘]
    - **ë°˜ë“œì‹œ êµ¬ê¸€ ê²€ìƒ‰(Google Search)ì„ ì‹¤í–‰**í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.
    - {current_date} ê¸°ì¤€, ìµœê·¼ 3ê°œì›” ì´ë‚´ì˜ ë‰´ìŠ¤ ìœ„ì£¼ë¡œ 5ê°œë¥¼ ì„ ì •í•˜ì„¸ìš”.
    - **ê²½ê³ : {one_year_ago} ì´ì „ì˜ ì˜¤ë˜ëœ ë‰´ìŠ¤ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
    - ê° ë‰´ìŠ¤ëŠ” ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì˜ ë§¨ ë§ˆì§€ë§‰ì— ì²¨ë¶€í•˜ì„¸ìš”.
    
    í˜•ì‹: <JSON_START> {{ "news": [ {{ "title_en": "...", "title_ko": "...", "link": "...", "sentiment": "ê¸ì •/ë¶€ì •/ì¼ë°˜", "date": "YYYY-MM-DD" }} ] }} <JSON_END>
    """
    try:
        response = model.generate_content(prompt)
        full_text = response.text
        biz_analysis = full_text.split("<JSON_START>")[0].strip()
        paragraphs = [p.strip() for p in biz_analysis.split('\n') if len(p.strip()) > 20]
        html_output = "".join([f'<p style="display:block; text-indent:14px; margin-bottom:20px; line-height:1.8; text-align:justify; font-size: 15px; color: #333;">{p}</p>' for p in paragraphs])
        
        news_list = []
        if "<JSON_START>" in full_text:
            try:
                json_str = full_text.split("<JSON_START>")[1].split("<JSON_END>")[0].strip()
                news_list = json.loads(json_str).get("news", [])
            except: pass

        batch_upsert("analysis_cache", [{
            "cache_key": cache_key,
            "content": json.dumps({"html": html_output, "news": news_list}, ensure_ascii=False),
            "updated_at": datetime.now().isoformat()
        }], on_conflict="cache_key")
        return True
    except: return False

# (Tab 3) ì¬ë¬´ ë¶„ì„ AI
def run_tab3_analysis(ticker, company_name, metrics):
    if not model: return False
    cache_key = f"{ticker}_Financial_Report_Tab3"
    prompt = f"""
    ë‹¹ì‹ ì€ CFA ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {company_name} ({ticker}) íˆ¬ì ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    [ì¬ë¬´ ë°ì´í„°] {metrics}
    [ê°€ì´ë“œ]
    - ì–¸ì–´: í•œêµ­ì–´
    - í˜•ì‹: [Valuation], [Operating Performance], [Risk], [Conclusion] 4ê°œ ì†Œì œëª© ì‚¬ìš©.
    - ë¶„ëŸ‰: 10ì¤„ ë‚´ì™¸ ìš”ì•½.
    """
    try:
        response = model.generate_content(prompt)
        batch_upsert("analysis_cache", [{
            "cache_key": cache_key,
            "content": response.text,
            "updated_at": datetime.now().isoformat()
        }], on_conflict="cache_key")
        return True
    except: return False

# (Tab 4) ê¸°ê´€ í‰ê°€ AI
def run_tab4_analysis(ticker, company_name):
    if not model: return False
    cache_key = f"{ticker}_Tab4"
    prompt = f"""
    ë‹¹ì‹ ì€ IPO ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. Google ê²€ìƒ‰ì„ í†µí•´ {company_name} ({ticker})ì˜ ìµœì‹  ê¸°ê´€ ë¦¬í¬íŠ¸(Seeking Alpha, Renaissance Capital ë“±)ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
    [ì¶œë ¥ í¬ë§· JSON]
    <JSON_START>
    {{
        "rating": "Buy/Hold/Sell",
        "summary": "3ì¤„ ìš”ì•½ (í•œêµ­ì–´)",
        "pro_con": "**ê¸ì •**: ... \\n **ë¶€ì •**: ...",
        "links": [ {{"title": "Title", "link": "URL"}} ]
    }}
    <JSON_END>
    """
    try:
        response = model.generate_content(prompt)
        match = re.search(r'<JSON_START>(.*?)<JSON_END>', response.text, re.DOTALL)
        if match:
            batch_upsert("analysis_cache", [{
                "cache_key": cache_key,
                "content": match.group(1),
                "updated_at": datetime.now().isoformat()
            }], on_conflict="cache_key")
            return True
    except: return False

# (Tab 2) ê±°ì‹œ ì§€í‘œ ì—…ë°ì´íŠ¸
def update_macro_data(df):
    if not model: return
    print("ğŸŒ ê±°ì‹œ ì§€í‘œ(Tab 2) ì—…ë°ì´íŠ¸ ì¤‘...")
    cache_key = "Market_Dashboard_Metrics_Tab2"
    data = {"ipo_return": 15.2, "ipo_volume": len(df), "vix": 14.5, "fear_greed": 60} 
    try:
        prompt = f"í˜„ì¬ ì‹œì¥ ë°ì´í„°(VIX: {data['vix']:.2f}, IPOìˆ˜ìµë¥ : {data['ipo_return']:.1f}%)ë¥¼ ë°”íƒ•ìœ¼ë¡œ IPO íˆ¬ììì—ê²Œ ì£¼ëŠ” 3ì¤„ ì¡°ì–¸ (í•œêµ­ì–´)."
        ai_resp = model.generate_content(prompt).text
        batch_upsert("analysis_cache", [{"cache_key": "Global_Market_Dashboard_Tab2", "content": ai_resp, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
        batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": json.dumps(data), "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
    except Exception as e:
        print(f"Macro Fail: {e}")
        
# ==========================================
# [5] ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    print(f"ğŸš€ Worker Start: {datetime.now()}")
    df = get_target_stocks()
    if df.empty: return

    # 1. ì¶”ì  ëª…ë‹¨
    stock_list = [{"symbol": str(row['symbol']), "name": str(row['name']), "updated_at": datetime.now().isoformat()} for _, row in df.iterrows()]
    batch_upsert("stock_cache", stock_list, on_conflict="symbol")

    # 2. ì£¼ê°€/ìƒíƒœ
    update_all_prices_batch(df)

    # 3. AI ë¶„ì„ ë£¨í”„
    for idx, row in df.iterrows():
        print(f"[{idx+1}/{len(df)}] {row['symbol']} ë¶„ì„ ì¤‘...")
        run_tab1_analysis(row['symbol'], row['name'])
        run_tab0_analysis(row['symbol'], row['name'])
        time.sleep(1.5)

if __name__ == "__main__":
    main()
