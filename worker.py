import os
import time
import json
import re
import requests
import pandas as pd
import numpy as np
import yfinance as yf
import logging
from datetime import datetime, timedelta

from supabase import create_client
import google.generativeai as genai

# ==========================================
# [1] í™˜ê²½ ì„¤ì •
# ==========================================
raw_url = os.environ.get("SUPABASE_URL", "")
if "/rest/v1" in raw_url:
    SUPABASE_URL = raw_url.split("/rest/v1")[0].rstrip('/')
else:
    SUPABASE_URL = raw_url.rstrip('/')

SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")
GENAI_API_KEY = os.environ.get("GENAI_API_KEY", "")
FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY", "")

logging.getLogger('yfinance').setLevel(logging.CRITICAL)

if not (SUPABASE_URL and SUPABASE_KEY):
    print("âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½ (SUPABASE_URL ë˜ëŠ” KEY)")
    exit()

try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    print(f"âŒ Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    exit()

model = None 
if GENAI_API_KEY:
    genai.configure(api_key=GENAI_API_KEY)
    try:
        model = genai.GenerativeModel('gemini-2.0-flash', tools=[{'google_search_retrieval': {}}])
        print("âœ… AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ (Search Tool í™œì„±í™”)")
    except:
        model = genai.GenerativeModel('gemini-2.0-flash')
        print("âš ï¸ AI ëª¨ë¸ ê¸°ë³¸ ë¡œë“œ (Search Tool ì œì™¸)")

SUPPORTED_LANGS = {
    'ko': 'ì „ë¬¸ì ì¸ í•œêµ­ì–´(Korean)',
    'en': 'Professional English',
    'ja': 'å°‚é–€çš„ãªæ—¥æœ¬èª(Japanese)'
}

# ==========================================
# [2] í—¬í¼ í•¨ìˆ˜
# ==========================================
def sanitize_value(v):
    if v is None or pd.isna(v): return None
    if isinstance(v, (np.floating, float)):
        return float(v) if not (np.isinf(v) or np.isnan(v)) else 0.0
    if isinstance(v, (np.integer, int)): return int(v)
    if isinstance(v, (np.bool_, bool)): return bool(v)
    return str(v).strip().replace('\x00', '')

def batch_upsert(table_name, data_list, on_conflict="ticker"):
    if not data_list: return
    endpoint = f"{SUPABASE_URL}/rest/v1/{table_name}?on_conflict={on_conflict}"
    headers = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "return=minimal,resolution=merge-duplicates" 
    }
    clean_batch = []
    for item in data_list:
        payload = {k: sanitize_value(v) for k, v in item.items()}
        if payload.get(on_conflict):
            clean_batch.append(payload)

    if not clean_batch: return

    try:
        resp = requests.post(endpoint, json=clean_batch, headers=headers)
        if resp.status_code in [200, 201, 204]:
            print(f"âœ… [{table_name}] {len(clean_batch)}ê°œ ì €ì¥ ì„±ê³µ")
        else:
            print(f"âŒ [{table_name}] ì €ì¥ ì‹¤íŒ¨ ({resp.status_code})")
    except Exception as e:
        print(f"âŒ [{table_name}] í†µì‹  ì—ëŸ¬: {e}")

def get_target_stocks():
    if not FINNHUB_API_KEY: return pd.DataFrame()
    now = datetime.now()
    ranges = [
        (now - timedelta(days=200), now + timedelta(days=35)), 
        (now - timedelta(days=380), now - timedelta(days=170)), 
        (now - timedelta(days=560), now - timedelta(days=350))
    ]
    all_data = []
    for start_dt, end_dt in ranges:
        url = f"https://finnhub.io/api/v1/calendar/ipo?from={start_dt.strftime('%Y-%m-%d')}&to={end_dt.strftime('%Y-%m-%d')}&token={FINNHUB_API_KEY}"
        try:
            res = requests.get(url, timeout=10).json()
            if res.get('ipoCalendar'): all_data.extend(res['ipoCalendar'])
        except: continue
        
    if not all_data: return pd.DataFrame()
    df = pd.DataFrame(all_data).dropna(subset=['symbol'])
    df['symbol'] = df['symbol'].astype(str).str.strip()
    return df.drop_duplicates(subset=['symbol'])


# ==========================================
# [3] AI ë¶„ì„ í•¨ìˆ˜ë“¤ (app.pyì™€ ì™„ë²½ ë™ê¸°í™”)
# ==========================================

def run_tab0_analysis(ticker, company_name):
    """Tab 0: ê³µì‹œ ë¶„ì„ (í”„ë¡¬í”„íŠ¸ ë¼ˆëŒ€ ë° v9 ìºì‹œí‚¤ ë™ê¸°í™”)"""
    if not model: return
    
    def_meta = {
        "S-1": {
            "points": "Risk Factors(íŠ¹ì´ ì†Œì†¡/ê·œì œ), Use of Proceeds(ìê¸ˆ ìš©ë„ì˜ ê±´ì „ì„±), MD&A(ì„±ì¥ ë™ì¸)",
            "structure": """
            [ë‚´ìš© êµ¬ì„± - ë°˜ë“œì‹œ 3ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìƒì„¸í•˜ê³  í’ì„±í•˜ê²Œ ì‘ì„±í•  ê²ƒ]
            1. **[íˆ¬ìí¬ì¸íŠ¸]** : í•´ë‹¹ ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ê°€ì¥ ì¤‘ìš”í•œ íˆ¬ì í¬ì¸íŠ¸ë¥¼ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê·¼ê±°ì™€ í•¨ê»˜ ìƒì„¸íˆ ì„œìˆ í•˜ì„¸ìš”.
            2. **[ì„±ì¥ê°€ëŠ¥ì„±]** : MD&A(ê²½ì˜ì§„ ë¶„ì„)ë¥¼ í†µí•´ ë³¸ ê¸°ì—…ì˜ ì‹¤ì§ˆì  ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ ì¬ë¬´ì  í•¨ì˜ë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì„¸ìš”.
            3. **[í•µì‹¬ë¦¬ìŠ¤í¬]** : íˆ¬ììê°€ ë°˜ë“œì‹œ ê²½ê³„í•´ì•¼ í•  í•µì‹¬ ë¦¬ìŠ¤í¬ 1ê°€ì§€ì™€ ê·¸ íŒŒê¸‰ íš¨ê³¼ ë° ëŒ€ì‘ì±…ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.
            """
        },
        "424B4": {
            "points": "Underwriting(ì£¼ê´€ì‚¬ ë“±ê¸‰), Final Price(ê¸°ê´€ ë°°ì • ë¬¼ëŸ‰), IPO Outcome(ìµœì¢… ê³µëª¨ ê²°ê³¼)",
            "structure": """
            [ë‚´ìš© êµ¬ì„± - ë°˜ë“œì‹œ 3ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìƒì„¸í•˜ê³  í’ì„±í•˜ê²Œ ì‘ì„±í•  ê²ƒ]
            1. **[ìµœì¢…ê³µëª¨ê°€]** : í™•ì •ëœ ê³µëª¨ê°€ê°€ í¬ë§ ë°´ë“œ ìƒë‹¨ì¸ì§€ í•˜ë‹¨ì¸ì§€ ë¶„ì„í•˜ê³ , ê·¸ ì˜ë¯¸(ì‹œì¥ ìˆ˜ìš”)ë¥¼ í•´ì„í•˜ì„¸ìš”.
            2. **[ìê¸ˆí™œìš©]** : í™•ì •ëœ ì¡°ë‹¬ ìê¸ˆì´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ìš°ì„ ìˆœìœ„ ì‚¬ì—…ì— íˆ¬ì…ë  ì˜ˆì •ì¸ì§€ ìµœì¢… ì ê²€í•˜ì„¸ìš”.
            3. **[ìƒì¥í›„ ì „ë§]** : ì£¼ê´€ì‚¬ë‹¨ êµ¬ì„±ê³¼ ë°°ì • ë¬¼ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì¥ ì´ˆê¸° ìœ í†µ ë¬¼ëŸ‰ ë¶€ë‹´ì´ë‚˜ ë³€ë™ì„±ì„ ì˜ˆì¸¡í•˜ì„¸ìš”.
            """
        }
    }

    # ì›Œì»¤ì—ì„œëŠ” í•µì‹¬ ì„œë¥˜ 2ê°€ì§€ë§Œ ìš°ì„  ì²˜ë¦¬ (ë¹„ìš©/ì‹œê°„ ì ˆì•½)
    for topic in ["S-1", "424B4"]:
        curr_meta = def_meta[topic]
        
        for lang_code, target_lang in SUPPORTED_LANGS.items():
            # ğŸš¨ [í•µì‹¬] app.pyì™€ ë™ì¼í•œ ìºì‹œ í‚¤ ì ìš© (v9)
            cache_key = f"{company_name}_{topic}_Tab0_v9_{lang_code}"
            
            if lang_code == 'en':
                labels = ["Analysis Target", "Instructions", "Structure & Format", "Writing Style Guide"]
                role_desc = "You are a professional senior analyst from Wall Street."
                no_intro_prompt = 'CRITICAL: NEVER introduce yourself. DO NOT include Korean translations in headings. START IMMEDIATELY with the first English **[Heading]**.'
                lang_directive = "The guide below is in Korean for reference, but you MUST translate all headings and content into English."
            elif lang_code == 'ja':
                labels = ["åˆ†æå¯¾è±¡", "æŒ‡é‡", "å†…å®¹æ§‹æˆãŠã‚ˆã³å½¢å¼", "æ–‡ä½“ã‚¬ã‚¤ãƒ‰"]
                role_desc = "ã‚ãªãŸã¯ã‚¦ã‚©ãƒ¼ãƒ«è¡—å‡ºèº«ã®å°‚é–€åˆ†æå®¶ã§ã™ã€‚"
                no_intro_prompt = 'ã€é‡è¦ã€‘è‡ªå·±ç´¹ä»‹ã‚„æŒ¨æ‹¶ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™ã€‚è¦‹å‡ºã—ã«éŸ“å›½èªã‚’ä½µè¨˜ã—ãªã„ã§ãã ã•ã„ã€‚1æ–‡å­—ç›®ã‹ã‚‰ã„ããªã‚Šæ—¥æœ¬èªã®**[è¦‹å‡ºã—]**ã§æœ¬è«–ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„ã€‚'
                lang_directive = "æ§‹æˆ ê°€ì´ë“œëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì œê³µë˜ë‚˜, ëª¨ë“  ì œëª©ê³¼ ë‚´ìš©ì€ ë°˜ë“œì‹œ ì¼ë³¸ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”."
            else:
                labels = ["ë¶„ì„ ëŒ€ìƒ", "ì§€ì¹¨", "ë‚´ìš© êµ¬ì„± ë° í˜•ì‹ - ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥¼ ê²ƒ", "ë¬¸ì²´ ê°€ì´ë“œ"]
                role_desc = "ë‹¹ì‹ ì€ ì›”ê°€ ì¶œì‹ ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤."
                no_intro_prompt = 'ìê¸°ì†Œê°œë‚˜ ì¸ì‚¬ë§, ì„œë¡ ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”. 1ê¸€ìë¶€í„° ë°”ë¡œ ë³¸ë¡ (**[ì†Œì œëª©]**)ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.'
                lang_directive = ""

            prompt = f"""
            {labels[0]}: {company_name} - {topic}
            {labels[1]} (Checkpoints): {curr_meta['points']}
            
            [{labels[1]}]
            {role_desc}
            {no_intro_prompt}
            {lang_directive}
            
            [{labels[2]}]
            ê° ë¬¸ë‹¨ì˜ ì‹œì‘ì— í•´ë‹¹ ì–¸ì–´ë¡œ ë²ˆì—­ëœ **[ì†Œì œëª©]**ì„ ë¶™ì—¬ì„œ ë‚´ìš©ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ê³  êµµì€ ê¸€ì”¨ë¥¼ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”.
            {curr_meta['structure']}

            [{labels[3]}]
            - ë°˜ë“œì‹œ '{target_lang}'ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. (ì ˆëŒ€ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì„ì§€ ë§ˆì„¸ìš”)
            - ë¬¸ì¥ ëì´ ëŠê¸°ì§€ ì•Šë„ë¡ ë§¤ë„ëŸ½ê²Œ ì—°ê²°í•˜ì„¸ìš”.
            """
            try:
                response = model.generate_content(prompt)
                batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": response.text, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
                time.sleep(1)
            except: pass

def run_tab1_analysis(ticker, company_name):
    """Tab 1: ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ì•½ ë° ë‰´ìŠ¤ (v2 ìºì‹œí‚¤ ë™ê¸°í™”)"""
    if not model: return False
    now = datetime.now()
    current_date = now.strftime("%Y-%m-%d")
    
    LANG_MAP = {
        'ko': 'ì „ë¬¸ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´(Korean)',
        'en': 'Professional English',
        'ja': 'æ¥µã‚ã¦è‡ªç„¶ã§å°‚é–€çš„ãªæ—¥æœ¬èª(Japanese)'
    }
    
    for lang_code, _ in SUPPORTED_LANGS.items():
        # ğŸš¨ [í•µì‹¬] app.pyì™€ ë™ì¼í•œ ìºì‹œ í‚¤ ì ìš© (v2)
        cache_key = f"{ticker}_Tab1_v2_{lang_code}"
        target_lang = LANG_MAP.get(lang_code, 'í•œêµ­ì–´')
        
        if lang_code == 'ja':
            task1_label = "[ã‚¿ã‚¹ã‚¯1: ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ·±å±¤åˆ†æ]"
            task2_label = "[ã‚¿ã‚¹ã‚¯2: æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åé›†]"
            lang_instruction = "å¿…ãšè‡ªç„¶ãªæ—¥æœ¬èªã®ã¿ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚éŸ“å›½ì–´ë‚˜ ì˜ì–´ ë‹¨ì–´ë¥¼ ì„ì§€ ë§ˆì„¸ìš”. ê¸°ì—…ëª…ë§Œ ì˜ì–´ë¥¼ í—ˆìš©í•©ë‹ˆë‹¤."
        elif lang_code == 'en':
            task1_label = "[Task 1: Deep Business Model Analysis]"
            task2_label = "[Task 2: Latest News Collection]"
            lang_instruction = "Your entire response MUST be in English only. Do not use any Korean."
        else:
            task1_label = "[ì‘ì—… 1: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì‹¬ì¸µ ë¶„ì„]"
            task2_label = "[ì‘ì—… 2: ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘]"
            lang_instruction = "ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."

        prompt = f"""
        ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ ì¦ê¶Œì‚¬ ë¦¬ì„œì¹˜ ì„¼í„°ì˜ ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë¶„ì„ ëŒ€ìƒ: {company_name} ({ticker}) ì˜¤ëŠ˜ ë‚ ì§œ: {current_date}

        {task1_label}
        1. ì–¸ì–´: {lang_instruction}
        2. í¬ë§·: ë°˜ë“œì‹œ 3ê°œì˜ ë¬¸ë‹¨(ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸, ì¬ë¬´ í˜„í™©, í–¥í›„ ì „ë§)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì„¸ìš”.
        3. ê¸ˆì§€: ì œëª©, ì†Œì œëª©, íŠ¹ìˆ˜ê¸°í˜¸, ë¶ˆë ›í¬ì¸íŠ¸(-)ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”. ì¸ì‚¬ë§ ì—†ì´ ë°”ë¡œ ë³¸ë¡ ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.

        {task2_label}
        - ë°˜ë“œì‹œ êµ¬ê¸€ ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ì—¬ ìµœê·¼ 1ë…„ ì´ë‚´ì˜ ë‰´ìŠ¤ 5ê°œë¥¼ ì„ ì •í•˜ì„¸ìš”.
        - [ì¤‘ìš”] sentiment ê°’ì€ ì‹œìŠ¤í…œ ë¡œì§ì„ ìœ„í•´ ë¬´ì¡°ê±´ "ê¸ì •", "ë¶€ì •", "ì¼ë°˜" ì¤‘ í•˜ë‚˜ë¥¼ í•œêµ­ì–´ë¡œ ì ìœ¼ì„¸ìš”.
        í˜•ì‹: <JSON_START> {{ "news": [ {{ "title_en": "Original English Title", "translated_title": "{target_lang}ë¡œ ë²ˆì—­ëœ ì œëª©", "link": "...", "sentiment": "ê¸ì •/ë¶€ì •/ì¼ë°˜", "date": "YYYY-MM-DD" }} ] }} <JSON_END>
        """
        try:
            response = model.generate_content(prompt)
            full_text = response.text
            
            biz_analysis = full_text.split("<JSON_START>")[0].strip()
            biz_analysis = re.sub(r'#.*', '', biz_analysis).strip()
            paragraphs = [p.strip() for p in biz_analysis.split('\n') if len(p.strip()) > 20]
            
            indent_size = "14px" if lang_code == "ko" else "0px"
            html_output = "".join([f'<p style="display:block; text-indent:{indent_size}; margin-bottom:20px; line-height:1.8; text-align:justify; font-size: 15px; color: #333;">{p}</p>' for p in paragraphs])
            
            news_list = []
            if "<JSON_START>" in full_text:
                try: 
                    json_part = full_text.split("<JSON_START>")[1].split("<JSON_END>")[0].strip()
                    news_list = json.loads(json_part).get("news", [])
                except: pass
                
            batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": json.dumps({"html": html_output, "news": news_list}, ensure_ascii=False), "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
            time.sleep(1.5)
        except: pass

def run_tab3_analysis(ticker, company_name, metrics):
    """Tab 3: ì¬ë¬´ ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸"""
    if not model: return False
    for lang_code, target_lang in SUPPORTED_LANGS.items():
        cache_key = f"{ticker}_Financial_Report_Tab3_{lang_code}"
        
        prompt = f"""
        ë‹¹ì‹ ì€ CFA ìê²©ì„ ë³´ìœ í•œ ìˆ˜ì„ ì£¼ì‹ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        ì•„ë˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {company_name} ({ticker})ì— ëŒ€í•œ íˆ¬ì ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        ì¬ë¬´ ë°ì´í„°: {metrics}

        [ì‘ì„± ê°€ì´ë“œ]
        1. ì–¸ì–´: ë°˜ë“œì‹œ '{target_lang}'ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        2. í˜•ì‹: ì•„ë˜ 4ê°€ì§€ ì†Œì œëª©ì„ ë°˜ë“œì‹œ ì‚¬ìš©í•˜ì—¬ ë‹¨ë½ì„ êµ¬ë¶„í•˜ì„¸ìš”. (ì†Œì œëª© ìì²´ë„ {target_lang}ì— ë§ê²Œ ë²ˆì—­í•˜ì„¸ìš”)
           **[Valuation & Market Position]**
           **[Operating Performance]**
           **[Risk & Solvency]**
           **[Analyst Conclusion]**
        3. ë‚´ìš©: ìˆ˜ì¹˜ë¥¼ ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³ , ìˆ˜ì¹˜ê°€ ê°–ëŠ” í•¨ì˜(í”„ë¦¬ë¯¸ì—„, íš¨ìœ¨ì„±, ë¦¬ìŠ¤í¬ ë“±)ë¥¼ í•´ì„í•˜ì„¸ìš”. ë¶„ëŸ‰ì€ 10~12ì¤„ ë‚´ì™¸ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.
        """
        try:
            response = model.generate_content(prompt)
            batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": response.text, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
            time.sleep(1)
        except: pass

def run_tab4_analysis(ticker, company_name):
    """Tab 4: ì›”ê°€ ê¸°ê´€ ë¶„ì„ (ê°•ë ¥ íŒŒì‹± ë²„ì „ ë™ê¸°í™”)"""
    if not model: return False
    
    LANG_MAP = {'ko': 'í•œêµ­ì–´ (Korean)', 'en': 'ì˜ì–´ (English)', 'ja': 'ì¼ë³¸ì–´ (Japanese)'}
    
    for lang_code, _ in SUPPORTED_LANGS.items():
        cache_key = f"{ticker}_Tab4_{lang_code}"
        target_lang = LANG_MAP.get(lang_code, 'í•œêµ­ì–´ (Korean)')
        lang_instruction = f"Respond strictly in {target_lang}."
        if lang_code == 'ja': lang_instruction = "å¿…ãšæ—¥æœ¬èª(Japanese)ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"

        prompt = f"""
        ë‹¹ì‹ ì€ ì›”ê°€ ì¶œì‹ ì˜ IPO ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
        êµ¬ê¸€ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ {company_name} ({ticker})ì— ëŒ€í•œ ìµœì‹  ê¸°ê´€ ë¦¬í¬íŠ¸(Seeking Alpha, Renaissance Capital ë“±)ë¥¼ ì°¾ì•„ ì‹¬ì¸µ ë¶„ì„í•˜ì„¸ìš”.

        [ì‘ì„± ì§€ì¹¨]
        1. ì–¸ì–´: ë°˜ë“œì‹œ '{target_lang}'ë¡œ ë‹µë³€í•˜ì„¸ìš”. {lang_instruction}
        2. ë¶„ì„ ê¹Šì´: ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ì´ ì•„ë‹Œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê·¼ê±°ë¥¼ ë“¤ì–´ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
        3. Pros & Cons: ê¸ì •ì  ìš”ì†Œ(Pros) 2ê°€ì§€ì™€ ë¶€ì •ì  ìš”ì†Œ(Cons) 2ê°€ì§€ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì„œìˆ í•˜ì„¸ìš”.
        4. Rating: ë°˜ë“œì‹œ (Strong Buy/Buy/Hold/Sell) ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒí•˜ì„¸ìš”. (ì´ ê°’ì€ ì˜ì–´ë¡œ ìœ ì§€)
        5. Summary: ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ 5ì¤„ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
        6. ë§í¬ ìœ„ì¹˜ êµ¬ë¶„: ë³¸ë¬¸ ì•ˆì—ëŠ” ì ˆëŒ€ URLì„ ë„£ì§€ ë§ê³ , ë°˜ë“œì‹œ "links" ë¦¬ìŠ¤íŠ¸ ì•ˆì—ë§Œ ì •í™•íˆ ê¸°ì…í•˜ì„¸ìš”.

        <JSON_START>
        {{
            "rating": "Buy/Hold/Sell ì¤‘ í•˜ë‚˜",
            "summary": "{target_lang}ì— ë§ê²Œ ë²ˆì—­ëœ 3ì¤„ ìš”ì•½ ë‚´ìš©",
            "pro_con": "**Pros**:\\n- ë‚´ìš©\\n\\n**Cons**:\\n- ë‚´ìš© (ì–¸ì–´: {target_lang})",
            "links": [ {{"title": "ê²€ìƒ‰ëœ ë¦¬í¬íŠ¸ ì œëª©", "link": "URL"}} ]
        }}
        <JSON_END>
        """
        try:
            response = model.generate_content(prompt)
            match = re.search(r'<JSON_START>(.*?)<JSON_END>', response.text, re.DOTALL)
            if match:
                clean_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', match.group(1).strip())
                batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": clean_str, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
            time.sleep(1)
        except: pass

def update_macro_data(df):
    """Tab 2: ê±°ì‹œ ì§€í‘œ ë¶„ì„ ì½”ë©˜íŠ¸"""
    if not model: return
    print("ğŸŒ ê±°ì‹œ ì§€í‘œ(Tab 2) ì—…ë°ì´íŠ¸ ì¤‘...")
    
    # ì›Œì»¤ì—ì„œëŠ” ê°„ë‹¨íˆ ê¸°ì´ˆ ë°ì´í„°ë§Œ êµ¬ì„±í•˜ì—¬ AI ì½”ë©˜íŠ¸ë§Œ ìºì‹±í•´ë‘¡ë‹ˆë‹¤. (ì‹¤ì œ ìˆ˜ì¹˜ ê³„ì‚°ì€ app.pyê°€ ë‹´ë‹¹)
    data = {"ipo_return": 15.2, "ipo_volume": len(df), "vix": 14.5, "fear_greed": 60} 
    
    for lang_code, target_lang in SUPPORTED_LANGS.items():
        cache_key_report = f"Global_Market_Dashboard_Tab2_{lang_code}"
        try:
            prompt = f"""
            ë‹¹ì‹ ì€ ì›”ê°€ì˜ ìˆ˜ì„ ì‹œì¥ ì „ëµê°€(Chief Market Strategist)ì…ë‹ˆë‹¤.
            í˜„ì¬ ì‹œì¥ ë°ì´í„°(VIX: {data['vix']:.2f}, IPOìˆ˜ìµë¥ : {data['ipo_return']:.1f}%) ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ê³¼ IPO ì‹œì¥ì˜ ìƒíƒœë¥¼ ì§„ë‹¨í•˜ëŠ” ì¼ì¼ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

            [ì‘ì„± ê°€ì´ë“œ]
            - ì–¸ì–´: ë°˜ë“œì‹œ '{target_lang}'ë¡œ ì‘ì„±í•˜ì„¸ìš”. (ë‹¤ë¥¸ ì–¸ì–´ ì ˆëŒ€ í˜¼ìš© ê¸ˆì§€)
            - í˜•ì‹: ì¤„ê¸€ë¡œ ëœ 3~5ì¤„ì˜ ìš”ì•½ ë¦¬í¬íŠ¸ë¡œ ì œëª©, ì†Œì œëª©, í—¤ë”(##), ì¸ì‚¬ë§ì„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
            """
            ai_resp = model.generate_content(prompt).text
            # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì •ì œ
            ai_resp = re.sub(r'^#+.*$', '', ai_resp, flags=re.MULTILINE).strip()
            
            batch_upsert("analysis_cache", [{"cache_key": cache_key_report, "content": ai_resp, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
            time.sleep(1)
        except: pass

# ==========================================
# [4] ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ==========================================
def main():
    print(f"ğŸš€ Worker Start: {datetime.now()}")
    
    df = get_target_stocks()
    if df.empty: 
        print("âš ï¸ ìˆ˜ì§‘ëœ IPO ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\nğŸ“‹ [stock_cache] ëª…ë‹¨ ì—…ë°ì´íŠ¸ ì‹œì‘...")
    now_iso = datetime.now().isoformat()
    stock_list = []
    
    for _, row in df.iterrows():
        stock_list.append({
            "symbol": str(row['symbol']),
            "name": str(row['name']) if pd.notna(row['name']) else "Unknown",
            "last_updated": now_iso 
        })
    
    batch_upsert("stock_cache", stock_list, on_conflict="symbol")

    update_macro_data(df)
    
    total = len(df)
    print(f"\nğŸ¤– AI ì‹¬ì¸µ ë¶„ì„ ì‹œì‘ (ì´ {total}ê°œ ì¢…ëª©)...")
    
    for idx, row in df.iterrows():
        symbol = row.get('symbol')
        name = row.get('name')
        listing_date = row.get('date')
        
        is_old = False
        try:
            if (datetime.now() - datetime.strptime(str(listing_date), "%Y-%m-%d")).days > 365: 
                is_old = True
        except: pass
        
        is_full_update = (datetime.now().weekday() == 0 or not is_old)
        
        print(f"[{idx+1}/{total}] {symbol} ë¶„ì„ ì¤‘...", flush=True)
        
        try:
            run_tab1_analysis(symbol, name)
            
            if is_full_update:
                run_tab0_analysis(symbol, name)
                run_tab4_analysis(symbol, name)
                try:
                    tk = yf.Ticker(symbol)
                    run_tab3_analysis(symbol, name, {"pe": tk.info.get('forwardPE', 0)})
                except: pass
            
            time.sleep(1.2) 
            
        except Exception as e:
            print(f"âš ï¸ {symbol} ë¶„ì„ ê±´ë„ˆëœ€: {e}")
            continue
            
    print(f"\nğŸ ëª¨ë“  ì‘ì—… ì¢…ë£Œ: {datetime.now()}")

if __name__ == "__main__":
    main()
