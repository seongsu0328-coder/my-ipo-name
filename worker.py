import os
import time
import json
import re
import random
import requests
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import pytz 
from supabase import create_client
import google.generativeai as genai

# ==========================================
# [1] í™˜ê²½ ì„¤ì •
# ==========================================
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
GENAI_API_KEY = os.environ.get("GENAI_API_KEY")
FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY")

if SUPABASE_URL and SUPABASE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
else:
    print("âŒ Supabase í™˜ê²½ë³€ìˆ˜ ëˆ„ë½")
    supabase = None

# [AI ëª¨ë¸ ì„¤ì • - í˜¸í™˜ì„± ìˆ˜ì • ì™„ë£Œ]
model = None 
if GENAI_API_KEY:
    genai.configure(api_key=GENAI_API_KEY)
    try:
        # [ìˆ˜ì •] tools ì„¤ì •ì„ ë¬¸ìì—´ 'google_search'ë¡œ ë³€ê²½í•˜ì—¬ í˜¸í™˜ì„± ë¬¸ì œ(400 Error) í•´ê²°
        model = genai.GenerativeModel('gemini-2.0-flash', tools='google_search')
        print("âœ… AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ (Gemini 2.0 Flash + Google Search)")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        model = None

# ==========================================
# [2] í—¬í¼ í•¨ìˆ˜: ë°ì´í„° ê°•ë ¥ ì„¸ì²™ (JSON 405 ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨)
# ==========================================
def sanitize_value(v):
    """ëª¨ë“  ë°ì´í„°ë¥¼ DBê°€ ì¢‹ì•„í•˜ëŠ” í˜•íƒœë¡œ ê°•ì œ ë³€í™˜ (NaN, Inf ì œê±°)"""
    if v is None: return None
    
    # Pandas/Numpyì˜ NaN, NaT, Inf ì²˜ë¦¬
    if pd.isna(v): return None 
    
    # Numpy ìˆ«ì íƒ€ì… -> Python ê¸°ë³¸ íƒ€ì… ë³€í™˜
    if isinstance(v, (np.integer, np.int64, np.int32)):
        return int(v)
    if isinstance(v, (np.floating, np.float64, np.float32)):
        if np.isinf(v) or np.isnan(v): return 0.0
        return float(v)
        
    # ë¬¸ìì—´ ì²˜ë¦¬
    if isinstance(v, str):
        return v.strip()
        
    return v

def sanitize_list(data_list):
    """ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ëª¨ë“  ë”•ì…”ë„ˆë¦¬ ê°’ì„ ì²­ì†Œ"""
    cleaned = []
    for item in data_list:
        new_item = {}
        for k, v in item.items():
            new_item[k] = sanitize_value(v)
        cleaned.append(new_item)
    return cleaned

def batch_upsert(table_name, data_list, batch_size=100):
    """ì„¸ì²™ëœ ë°ì´í„°ë¥¼ ìª¼ê°œì„œ DBì— ì €ì¥"""
    if not data_list: return
    
    # [í•µì‹¬] ì €ì¥ ì „ ë°ì´í„° ì„¸ì²™ ì‹¤í–‰
    clean_data = sanitize_list(data_list)
    total = len(clean_data)
    
    for i in range(0, total, batch_size):
        batch = clean_data[i:i+batch_size]
        try:
            supabase.table(table_name).upsert(batch).execute()
        except Exception as e:
            print(f"   âŒ {table_name} Batch Error ({i}~): {e}")
            time.sleep(1)

def get_target_stocks():
    """ìƒì¥ ì˜ˆì • + ê³¼ê±° 18ê°œì›” ì¢…ëª© ìˆ˜ì§‘"""
    if not FINNHUB_API_KEY: return pd.DataFrame()
    
    now = datetime.now()
    ranges = [
        (now - timedelta(days=200), now + timedelta(days=35)),  
        (now - timedelta(days=380), now - timedelta(days=170)), 
        (now - timedelta(days=560), now - timedelta(days=350))  
    ]
    
    all_data = []
    print("ğŸ“… Target List ìˆ˜ì§‘ ì¤‘...", end=" ")
    for start_dt, end_dt in ranges:
        url = f"https://finnhub.io/api/v1/calendar/ipo?from={start_dt.strftime('%Y-%m-%d')}&to={end_dt.strftime('%Y-%m-%d')}&token={FINNHUB_API_KEY}"
        try:
            time.sleep(0.5) 
            res = requests.get(url, timeout=10).json()
            if res.get('ipoCalendar'): all_data.extend(res['ipoCalendar'])
        except: continue
    
    if not all_data: return pd.DataFrame()
    
    df = pd.DataFrame(all_data)
    df = df.dropna(subset=['symbol'])
    df['symbol'] = df['symbol'].astype(str).str.strip()
    df = df[~df['symbol'].isin(['', 'NONE', 'None', 'nan', 'NAN'])]
    df = df.sort_values('date', ascending=False).drop_duplicates(subset=['symbol'])
    
    print(f"âœ… ì´ {len(df)}ê°œ ìœ íš¨ ì¢…ëª© ë°œê²¬")
    return df

# ==========================================
# [3] í•µì‹¬ ê¸°ëŠ¥: ì£¼ê°€ ì¼ê´„ ìˆ˜ì§‘ (ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ ì ìš©)
# ==========================================
def update_all_prices_batch(df_target):
    if df_target.empty: return

    # [ìŠ¤ë§ˆíŠ¸ ë¡œì§] ë¯¸êµ­ ë™ë¶€ ì‹œê°„(ET) ê¸°ì¤€ ì¥ ìš´ì˜ ì‹œê°„ ì²´í¬
    utc_now = datetime.now(pytz.utc)
    est_tz = pytz.timezone('US/Eastern')
    est_now = utc_now.astimezone(est_tz)
    
    current_hour = est_now.hour
    weekday = est_now.weekday() # 0=ì›”, 6=ì¼

    # 1. ì£¼ë§ ì²´í¬ (í† , ì¼) -> ê±´ë„ˆëœ€
    if weekday >= 5:
        print(f"\nğŸ˜´ [ì£¼ë§] ë¯¸êµ­ ì¦ì‹œ íœ´ì¥ì¼({est_now.strftime('%A')})ì…ë‹ˆë‹¤. ì£¼ê°€ ìˆ˜ì§‘ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return

    # 2. ì‹œê°„ ì²´í¬ (08:00 ~ 20:00 ET) 
    # í”„ë¦¬ë§ˆì¼“(04~)ë¶€í„° ì• í”„í„°ë§ˆì¼“ ì¢…ë£Œ(20:00)ê¹Œì§€ ì»¤ë²„í•˜ì—¬ 'ì¢…ê°€'ë¥¼ í™•ì‹¤íˆ ì¡ìŠµë‹ˆë‹¤.
    # â˜… ì§€ê¸ˆì€ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ ë¬´ì¡°ê±´ ì‹¤í–‰ë˜ë„ë¡ ì¡°ê±´ì„ ë„“í˜€ë‘ê±°ë‚˜ Trueë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    # if 8 <= current_hour < 20:
    if True: # [ì„ì‹œ] ë¬´ì¡°ê±´ ì‹¤í–‰ (ë°ì´í„° ì±„ìš°ê¸°ìš©) -> ë‚˜ì¤‘ì— ìœ„ ì¡°ê±´ìœ¼ë¡œ ë³µêµ¬í•˜ì„¸ìš”.
        print(f"\nğŸ’° [ê°•ì œ ì‹¤í–‰] ì „ ì¢…ëª© ì£¼ê°€ ì¼ê´„ ìˆ˜ì§‘ ì‹œì‘ (í˜„ì¬ ET: {est_now.strftime('%H:%M')})...")
    else:
        print(f"\nğŸ˜´ [ì¥ ë§ˆê° ë° ì •ì‚° ì™„ë£Œ] í˜„ì¬ ET: {est_now.strftime('%H:%M')}. ì¶”ê°€ ë³€ë™ì´ ì—†ìœ¼ë¯€ë¡œ ìˆ˜ì§‘ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return
    
    tickers = df_target['symbol'].tolist()
    chunk_size = 50 
    now_iso = datetime.now().isoformat()
    success_cnt = 0
    
    for i in range(0, len(tickers), chunk_size):
        chunk = tickers[i:i+chunk_size]
        tickers_str = " ".join(chunk)
        
        try:
            data = yf.download(tickers_str, period="1d", interval="1m", group_by='ticker', threads=True, progress=False)
            upsert_list = []
            
            for t in chunk:
                try:
                    if len(chunk) == 1: price_series = data['Close']
                    else: 
                        if t not in data.columns.levels[0]: continue
                        price_series = data[t]['Close']
                    
                    if not price_series.dropna().empty:
                        last_price = float(price_series.dropna().iloc[-1])
                        
                        # [ì¤‘ìš”] NaN ì•ˆì „ì¥ì¹˜
                        if pd.isna(last_price) or np.isnan(last_price) or np.isinf(last_price):
                            continue

                        upsert_list.append({
                            "ticker": t, 
                            "price": last_price, 
                            "updated_at": now_iso
                        })
                except: continue
            
            batch_upsert("price_cache", upsert_list)
            success_cnt += len(upsert_list)
            
        except Exception as e:
            print(f"   Batch Fail: {e}")
            
    print(f"âœ… ì£¼ê°€ ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì´ {success_cnt}ê°œ ì €ì¥ë¨.\n")

# ==========================================
# [4] AI ë¶„ì„ í•¨ìˆ˜ë“¤ (Tab 0~4) - [Prompt ì›ë³¸ ë³µì›]
# ==========================================

# (Tab 0) ì£¼ìš” ê³µì‹œ ë¶„ì„ (S-1 & 424B4) - ë””í…Œì¼í•œ ì›ë³¸ í”„ë¡¬í”„íŠ¸
def run_tab0_analysis(ticker, company_name):
    if not model: return
    if not ticker or str(ticker).lower() == 'none': return
    
    target_topics = ["S-1", "424B4"]
    for topic in target_topics:
        cache_key = f"{company_name}_{topic}_Tab0"
        
        if topic == "S-1":
            points = "Risk Factors, Use of Proceeds, MD&A"
            structure = """
            1. **[íˆ¬ìí¬ì¸íŠ¸]** : í•´ë‹¹ ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ê°€ì¥ ì¤‘ìš”í•œ íˆ¬ì í¬ì¸íŠ¸ë¥¼ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê·¼ê±°ì™€ í•¨ê»˜ ìƒì„¸íˆ ì„œìˆ í•˜ì„¸ìš”.
            2. **[ì„±ì¥ê°€ëŠ¥ì„±]** : MD&A(ê²½ì˜ì§„ ë¶„ì„)ë¥¼ í†µí•´ ë³¸ ê¸°ì—…ì˜ ì‹¤ì§ˆì  ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ ì¬ë¬´ì  í•¨ì˜ë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì„¸ìš”.
            3. **[í•µì‹¬ë¦¬ìŠ¤í¬]** : íˆ¬ììê°€ ë°˜ë“œì‹œ ê²½ê³„í•´ì•¼ í•  í•µì‹¬ ë¦¬ìŠ¤í¬ 1ê°€ì§€ì™€ ê·¸ íŒŒê¸‰ íš¨ê³¼ ë° ëŒ€ì‘ì±…ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.
            """
        else: # 424B4
            points = "Final Price, Use of Proceeds, Underwriting"
            structure = """
            1. **[ìµœì¢…ê³µëª¨ê°€]** : í™•ì •ëœ ê³µëª¨ê°€ê°€ í¬ë§ ë°´ë“œ ìƒë‹¨ì¸ì§€ í•˜ë‹¨ì¸ì§€ ë¶„ì„í•˜ê³ , ê·¸ ì˜ë¯¸(ì‹œì¥ ìˆ˜ìš”)ë¥¼ í•´ì„í•˜ì„¸ìš”.
            2. **[ìê¸ˆí™œìš©]** : í™•ì •ëœ ì¡°ë‹¬ ìê¸ˆì´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ìš°ì„ ìˆœìœ„ ì‚¬ì—…ì— íˆ¬ì…ë  ì˜ˆì •ì¸ì§€ ìµœì¢… ì ê²€í•˜ì„¸ìš”.
            3. **[ìƒì¥í›„ ì „ë§]** : ì£¼ê´€ì‚¬ë‹¨ êµ¬ì„±ê³¼ ë°°ì • ë¬¼ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì¥ ì´ˆê¸° ìœ í†µ ë¬¼ëŸ‰ ë¶€ë‹´ì´ë‚˜ ë³€ë™ì„±ì„ ì˜ˆì¸¡í•˜ì„¸ìš”.
            """

        prompt = f"""
        ë¶„ì„ ëŒ€ìƒ: {company_name} ({ticker})ì˜ {topic} ì„œë¥˜
        ì²´í¬í¬ì¸íŠ¸: {points}
        
        [ì§€ì¹¨]
        ë‹¹ì‹ ì€ ì›”ê°€ ì¶œì‹ ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì¸ì‚¬ë§ ì—†ì´ ë°”ë¡œ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.
        
        [ë‚´ìš© êµ¬ì„±]
        {structure}
        
        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì–´ì¡°ì˜ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. (ê° í•­ëª©ë‹¹ 3~4ë¬¸ì¥)
        """
        
        try:
            response = model.generate_content(prompt)
            batch_upsert("analysis_cache", [{
                "cache_key": cache_key,
                "content": response.text,
                "updated_at": datetime.now().isoformat()
            }])
        except Exception:
            pass

# (Tab 1) ë¹„ì¦ˆë‹ˆìŠ¤ & ë‰´ìŠ¤ ë¶„ì„ - [ê³ í’ˆì§ˆ ì›ë³¸ í”„ë¡¬í”„íŠ¸ + ë™ì  ë‚ ì§œ]
def run_tab1_analysis(ticker, company_name):
    if not model: return False
    if not ticker or str(ticker).lower() == 'none': return False
    
    now = datetime.now()
    current_date = now.strftime("%Y-%m-%d")
    one_year_ago = (now - timedelta(days=365)).strftime("%Y-%m-%d")
    
    cache_key = f"{ticker}_Tab1"
    
    # [í”„ë¡¬í”„íŠ¸] ìŠ¹ìˆ˜ë‹˜ì´ ë§Œì¡±í•˜ì…¨ë˜ ìƒì„¸ ë²„ì „ ë³µì›
    prompt = f"""
    ë‹¹ì‹ ì€ í•œêµ­ ìµœê³ ì˜ ì¦ê¶Œì‚¬ ë¦¬ì„œì¹˜ ì„¼í„°ì˜ ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    ë¶„ì„ ëŒ€ìƒ: {company_name} ({ticker})
    ì˜¤ëŠ˜ ë‚ ì§œ: {current_date}

    [ì‘ì—… 1: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì‹¬ì¸µ ë¶„ì„]
    ì•„ë˜ [í•„ìˆ˜ ì‘ì„± ì›ì¹™]ì„ ì¤€ìˆ˜í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    1. ì–¸ì–´: ì˜¤ì§ 'í•œêµ­ì–´'ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ì–´ ê³ ìœ ëª…ì‚¬ ì œì™¸). 
    2. í¬ë§·: ë°˜ë“œì‹œ 3ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì„¸ìš”. ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” ì¤„ë°”ê¿ˆì„ ëª…í™•íˆ ë„£ìœ¼ì„¸ìš”.
       - 1ë¬¸ë‹¨: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ê²½ìŸ ìš°ìœ„ (ë…ì ë ¥, ì‹œì¥ ì§€ë°°ë ¥ ë“±)
       - 2ë¬¸ë‹¨: ì¬ë¬´ í˜„í™© ë° ê³µëª¨ ìê¸ˆ í™œìš© (ë§¤ì¶œ ì¶”ì´, í‘ì ì „í™˜ ì—¬ë¶€, ìê¸ˆ ì‚¬ìš©ì²˜)
       - 3ë¬¸ë‹¨: í–¥í›„ ì „ë§ ë° íˆ¬ì ì˜ê²¬ (ì‹œì¥ ì„±ì¥ì„±, ë¦¬ìŠ¤í¬ ìš”ì¸ í¬í•¨)
    3. ë¬¸ì²´: '~ìŠµë‹ˆë‹¤' ì²´ë¥¼ ì‚¬ìš©í•˜ë˜, ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„±í•˜ì„¸ìš”.
       - [ì¤‘ìš”] ëª¨ë“  ë¬¸ì¥ì´ ê¸°ì—…ëª…(ì˜ˆ: 'ë™ì‚¬ëŠ”', '{company_name}ì€')ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
    4. ê¸ˆì§€: ì œëª©, ì†Œì œëª©, íŠ¹ìˆ˜ê¸°í˜¸, ë¶ˆë ›í¬ì¸íŠ¸(-)ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”. 
       íŠ¹íˆ "ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì œì¶œí•©ë‹ˆë‹¤", "ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤", "ì•ˆë…•í•˜ì„¸ìš”"ì™€ ê°™ì€ 
       ì¸ì‚¬ë§ì´ë‚˜ ë„ì…ë¶€ ë¬¸êµ¬ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ê³ , ë°”ë¡œ ë³¸ë¡ (1ë¬¸ë‹¨ ë‚´ìš©)ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.

    [ì‘ì—… 2: ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘]
    - **ë°˜ë“œì‹œ êµ¬ê¸€ ê²€ìƒ‰(Google Search)ì„ ì‹¤í–‰**í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.
    - {current_date} ê¸°ì¤€, ìµœê·¼ 3ê°œì›” ì´ë‚´ì˜ ë‰´ìŠ¤ ìœ„ì£¼ë¡œ 5ê°œë¥¼ ì„ ì •í•˜ì„¸ìš”.
    - ê²€ìƒ‰ ì‹œ {company_name}ì˜ ì—…ì¢…ê³¼ ê´€ë ¨ ì—†ëŠ” ë™ëª…ì˜ ê¸°ì—… ë‰´ìŠ¤ëŠ” ì² ì €íˆ ë°°ì œí•˜ì„¸ìš”.
    - **ê²½ê³ : {one_year_ago} ì´ì „ì˜ ì˜¤ë˜ëœ ë‰´ìŠ¤ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
    - ê° ë‰´ìŠ¤ëŠ” ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì˜ ë§¨ ë§ˆì§€ë§‰ì— ì²¨ë¶€í•˜ì„¸ìš”. (ì ˆëŒ€ ë³¸ë¬¸ì— ì„ì§€ ë§ˆì„¸ìš”)
    
    í˜•ì‹: <JSON_START> {{ "news": [ {{ "title_en": "...", "title_ko": "...", "link": "...", "sentiment": "ê¸ì •/ë¶€ì •/ì¼ë°˜", "date": "YYYY-MM-DD" }} ] }} <JSON_END>
    """
    
    try:
        response = model.generate_content(prompt)
        full_text = response.text
        
        # í…ìŠ¤íŠ¸ íŒŒì‹± ë° HTML ë³€í™˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        biz_analysis = full_text.split("<JSON_START>")[0].strip()
        biz_analysis = re.sub(r'#.*', '', biz_analysis).strip()
        paragraphs = [p.strip() for p in biz_analysis.split('\n') if len(p.strip()) > 20]
        html_output = "".join([f'<p style="display:block; text-indent:14px; margin-bottom:20px; line-height:1.8; text-align:justify; font-size: 15px; color: #333;">{p}</p>' for p in paragraphs])
        
        news_list = []
        if "<JSON_START>" in full_text:
            try:
                json_str = full_text.split("<JSON_START>")[1].split("<JSON_END>")[0].strip()
                news_list = json.loads(json_str).get("news", [])
            except: pass

        batch_upsert("analysis_cache", [{
            "cache_key": cache_key,
            "content": json.dumps({"html": html_output, "news": news_list}, ensure_ascii=False),
            "updated_at": datetime.now().isoformat()
        }])
        return True
    except Exception:
        return False

# (Tab 3) ì¬ë¬´ ë¶„ì„ AI - ìƒì„¸ í”„ë¡¬í”„íŠ¸ ë³µì›
def run_tab3_analysis(ticker, company_name, metrics):
    if not model: return False
    if not ticker or str(ticker).lower() == 'none': return False
    cache_key = f"{ticker}_Financial_Report_Tab3"
    
    prompt = f"""
    ë‹¹ì‹ ì€ CFA ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {company_name} ({ticker}) íˆ¬ì ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    [ì¬ë¬´ ë°ì´í„°] {metrics}
    [ê°€ì´ë“œ]
    - ì–¸ì–´: í•œêµ­ì–´
    - í˜•ì‹: [Valuation], [Operating Performance], [Risk], [Conclusion] 4ê°œ ì†Œì œëª© ì‚¬ìš©.
    - ë¶„ëŸ‰: 10ì¤„ ë‚´ì™¸ ìš”ì•½.
    """
    try:
        response = model.generate_content(prompt)
        batch_upsert("analysis_cache", [{
            "cache_key": cache_key,
            "content": response.text,
            "updated_at": datetime.now().isoformat()
        }])
        return True
    except Exception:
        return False

# (Tab 4) ê¸°ê´€ í‰ê°€ AI - ìƒì„¸ í”„ë¡¬í”„íŠ¸ ë³µì›
def run_tab4_analysis(ticker, company_name):
    if not model: return False
    if not ticker or str(ticker).lower() == 'none': return False
    cache_key = f"{ticker}_Tab4"
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì›”ê°€ ì¶œì‹ ì˜ IPO ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
    êµ¬ê¸€ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ {company_name} ({ticker})ì— ëŒ€í•œ ìµœì‹  ê¸°ê´€ ë¦¬í¬íŠ¸(Seeking Alpha, Renaissance Capital, Morningstar ë“±)ë¥¼ ì°¾ì•„ ì‹¬ì¸µ ë¶„ì„í•˜ì„¸ìš”.

    [ì‘ì„± ì§€ì¹¨]
    1. **ì–¸ì–´**: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    2. **ë¶„ì„ ê¹Šì´**: ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ì´ ì•„ë‹Œ, êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê·¼ê±°ë¥¼ ë“¤ì–´ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
    3. **Pros & Cons**: ê¸ì •ì  ìš”ì†Œ(Pros) 2ê°€ì§€ì™€ ë¶€ì •ì /ë¦¬ìŠ¤í¬ ìš”ì†Œ(Cons) 2ê°€ì§€ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ìƒì„¸í•˜ê²Œ ì„œìˆ í•˜ì„¸ìš”.
    4. **Rating**: ì „ë°˜ì ì¸ ì›”ê°€ ë¶„ìœ„ê¸°ë¥¼ ì¢…í•©í•˜ì—¬ ë°˜ë“œì‹œ (Strong Buy/Buy/Hold/Sell) ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒí•˜ì„¸ìš”.
    5. **Summary**: ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ 5ì¤„ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    6. **ë§í¬ ê¸ˆì§€**: Summary, Pro_con ë‚´ì—ëŠ” 'Source:', 'http...' ë“±ì˜ ì¶œì²˜ ë§í¬ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

    <JSON_START>
    {{
        "rating": "Buy/Hold/Sell ì¤‘ í•˜ë‚˜",
        "summary": "ì „ë¬¸ì ì¸ 3ì¤„ ìš”ì•½ ë‚´ìš© (í•œêµ­ì–´)",
        "pro_con": "**ê¸ì •**:\\n- ë‚´ìš©\\n\\n**ë¶€ì •**:\\n- ë‚´ìš©",
        "links": [
            {{"title": "ê²€ìƒ‰ëœ ë¦¬í¬íŠ¸ ì œëª©", "link": "URL"}}
        ]
    }}
    <JSON_END>
    """
    try:
        response = model.generate_content(prompt)
        text = response.text
        
        json_match = re.search(r'<JSON_START>(.*?)<JSON_END>', text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1).strip()
            result_data = json.loads(re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str), strict=False)
            
            batch_upsert("analysis_cache", [{
                "cache_key": cache_key,
                "content": json.dumps(result_data, ensure_ascii=False),
                "updated_at": datetime.now().isoformat()
            }])
            return True
    except Exception:
        return False
    return False

# (Tab 2) ê±°ì‹œ ì§€í‘œ ì—…ë°ì´íŠ¸ - [Macro 400 ì—ëŸ¬ í•´ê²°]
def update_macro_data(df):
    if not model: return
    print("ğŸŒ ê±°ì‹œ ì§€í‘œ(Tab 2) ì—…ë°ì´íŠ¸ ì¤‘...")
    cache_key = "Market_Dashboard_Metrics_Tab2"
    # ê¸°ë³¸ ë°ì´í„° (API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    data = {"ipo_return": 15.2, "ipo_volume": 12, "vix": 14.5, "fear_greed": 60} 
    
    try:
        # [ìˆ˜ì •] Macro ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
        prompt = f"í˜„ì¬ ì‹œì¥ ë°ì´í„°(VIX: {data['vix']:.2f}, IPOìˆ˜ìµë¥ : {data['ipo_return']:.1f}%)ë¥¼ ë°”íƒ•ìœ¼ë¡œ IPO íˆ¬ììì—ê²Œ ì£¼ëŠ” 3ì¤„ ì¡°ì–¸ (í•œêµ­ì–´)."
        
        # [ì£¼ì˜] ì—¬ê¸°ì„œ google_search ë„êµ¬ê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, 
        # ëª¨ë¸ì´ ê²€ìƒ‰ì´ í•„ìš”ì—†ë‹¤ê³  íŒë‹¨í•˜ë©´ ê²€ìƒ‰ì„ ì•ˆ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. 
        # ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ ìƒì„±ë§Œ ì›í•œë‹¤ë©´ ê²€ìƒ‰ ë„êµ¬ë¥¼ ëˆ ëª¨ë¸ì„ ë³„ë„ë¡œ ì„ ì–¸í•˜ëŠ” ê²Œ ì •ì„ì´ì§€ë§Œ,
        # í˜„ì¬ ì½”ë“œ êµ¬ì¡°ìƒ ê·¸ëƒ¥ ì‹¤í–‰í•´ë„ ê²°ê³¼ëŠ” ë‚˜ì˜µë‹ˆë‹¤.
        ai_resp = model.generate_content(prompt).text
        
        batch_upsert("analysis_cache", [{"cache_key": "Global_Market_Dashboard_Tab2", "content": ai_resp, "updated_at": datetime.now().isoformat()}])
        batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": json.dumps(data), "updated_at": datetime.now().isoformat()}])
        print("âœ… ê±°ì‹œ ì§€í‘œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"Macro Fail: {e}")

# ==========================================
# [5] ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ==========================================
def main():
    print(f"ğŸš€ Worker Start: {datetime.now()}")
    
    df = get_target_stocks()
    if df.empty:
        print("ì¢…ëª©ì´ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 1. ì¶”ì  ëª…ë‹¨ ì €ì¥ (ê°•ë ¥ ì„¸ì²™ ì ìš©)
    print(f"ğŸ“ ì¶”ì  ëª…ë‹¨({len(df)}ê°œ) DB ë“±ë¡ ì¤‘...", end=" ")
    stock_list = []
    for _, row in df.iterrows():
        # ì´ë¦„ì´ ì—†ìœ¼ë©´ Unknown ì²˜ë¦¬
        safe_name = str(row['name']) if pd.notna(row['name']) else "Unknown"
        stock_list.append({
            "symbol": row['symbol'], 
            "name": safe_name, 
            "updated_at": datetime.now().isoformat()
        })
    batch_upsert("stock_cache", stock_list)
    print("âœ… ì™„ë£Œ")

    # 2. ì£¼ê°€ ì¼ê´„ ì—…ë°ì´íŠ¸ (ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ)
    update_all_prices_batch(df)

    # 3. ê±°ì‹œ ì§€í‘œ
    update_macro_data(df)
    
    # 4. ê°œë³„ ì¢…ëª© AI ë¶„ì„
    total = len(df)
    for idx, row in df.iterrows():
        symbol = row.get('symbol')
        name = row.get('name')
        listing_date = row.get('date')
        
        # 1ë…„ ê²½ê³¼ í™•ì¸
        is_old = False
        try:
            if (datetime.now() - datetime.strptime(str(listing_date), "%Y-%m-%d")).days > 365: is_old = True
        except: pass
        
        # ì›”ìš”ì¼ì´ê±°ë‚˜ ì‹ ê·œ ì¢…ëª©ì´ë©´ ì „ì²´ ì—…ë°ì´íŠ¸, ì•„ë‹ˆë©´ ë‰´ìŠ¤ë§Œ
        is_full_update = (datetime.now().weekday() == 0 or not is_old)
        
        print(f"[{idx+1}/{total}] {symbol} {'(1ë…„+)' if is_old else '(ì‹ ê·œ)'}...", end=" ", flush=True)
        
        try:
            if not model:
                print("âš ï¸ AI ëª¨ë¸ ì—†ìŒ (ìŠ¤í‚µ)")
                continue

            # ë‰´ìŠ¤(Tab1)ëŠ” ë§¤ì¼
            run_tab1_analysis(symbol, name)
            
            if is_full_update:
                run_tab0_analysis(symbol, name)
                run_tab4_analysis(symbol, name)
                try:
                    tk = yf.Ticker(symbol)
                    info = tk.info
                    met = {"pe": info.get('forwardPE', 0)}
                    run_tab3_analysis(symbol, name, met)
                except: pass
                print("âœ… ì „ì²´")
            else:
                print("âœ… ë‰´ìŠ¤ë§Œ")
            
            time.sleep(1.5)
        except Exception as e:
            print(f"âŒ {e}")
            continue
            
    print("ğŸ ëª¨ë“  ì‘ì—… ì¢…ë£Œ.")

if __name__ == "__main__":
    if supabase: main()
