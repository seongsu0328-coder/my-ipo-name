import os
import time
import json
import re
import requests
import pandas as pd
import numpy as np
import yfinance as yf
import logging
from datetime import datetime, timedelta

from supabase import create_client
import google.generativeai as genai

# ==========================================
# [1] í™˜ê²½ ì„¤ì •
# ==========================================
raw_url = os.environ.get("SUPABASE_URL", "")
if "/rest/v1" in raw_url:
    SUPABASE_URL = raw_url.split("/rest/v1")[0].rstrip('/')
else:
    SUPABASE_URL = raw_url.rstrip('/')

SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")
GENAI_API_KEY = os.environ.get("GENAI_API_KEY", "")
FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY", "")

logging.getLogger('yfinance').setLevel(logging.CRITICAL)

if not (SUPABASE_URL and SUPABASE_KEY):
    print("âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½")
    exit()

try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    print(f"âŒ Supabase ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    exit()

# ëª¨ë¸ ì´ì›í™”
search_model = None   
standard_model = None 

if GENAI_API_KEY:
    genai.configure(api_key=GENAI_API_KEY)
    try:
        search_model = genai.GenerativeModel('gemini-2.0-flash', tools=[{'google_search_retrieval': {}}])
        standard_model = genai.GenerativeModel('gemini-2.0-flash')
        print("âœ… AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ (Search / Standard ì´ì›í™”)")
    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

SUPPORTED_LANGS = {
    'ko': 'ì „ë¬¸ì ì¸ í•œêµ­ì–´(Korean)',
    'en': 'Professional English',
    'ja': 'å°‚é–€çš„ãªæ—¥æœ¬èª(Japanese)'
}

# ==========================================
# [2] í—¬í¼ í•¨ìˆ˜
# ==========================================
def sanitize_value(v):
    if v is None or pd.isna(v): return None
    if isinstance(v, (np.floating, float)):
        return float(v) if not (np.isinf(v) or np.isnan(v)) else 0.0
    if isinstance(v, (np.integer, int)): return int(v)
    if isinstance(v, (np.bool_, bool)): return bool(v)
    return str(v).strip().replace('\x00', '')

def batch_upsert(table_name, data_list, on_conflict="ticker"):
    if not data_list: return
    endpoint = f"{SUPABASE_URL}/rest/v1/{table_name}?on_conflict={on_conflict}"
    headers = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "return=minimal,resolution=merge-duplicates" 
    }
    clean_batch = []
    for item in data_list:
        payload = {k: sanitize_value(v) for k, v in item.items()}
        if payload.get(on_conflict):
            clean_batch.append(payload)

    if not clean_batch: return

    try:
        requests.post(endpoint, json=clean_batch, headers=headers)
    except Exception as e:
        print(f"âŒ [{table_name}] ì—…ë¡œë“œ ì—ëŸ¬: {e}")

def get_target_stocks():
    if not FINNHUB_API_KEY: return pd.DataFrame()
    now = datetime.now()
    ranges = [
        (now - timedelta(days=200), now + timedelta(days=35)), 
        (now - timedelta(days=380), now - timedelta(days=170)), 
        (now - timedelta(days=560), now - timedelta(days=350))
    ]
    all_data = []
    for start_dt, end_dt in ranges:
        url = f"https://finnhub.io/api/v1/calendar/ipo?from={start_dt.strftime('%Y-%m-%d')}&to={end_dt.strftime('%Y-%m-%d')}&token={FINNHUB_API_KEY}"
        try:
            res = requests.get(url, timeout=10).json()
            if res.get('ipoCalendar'): all_data.extend(res['ipoCalendar'])
        except: continue
        
    if not all_data: return pd.DataFrame()
    df = pd.DataFrame(all_data).dropna(subset=['symbol'])
    df['symbol'] = df['symbol'].astype(str).str.strip()
    return df.drop_duplicates(subset=['symbol'])

# ğŸ’¡ [ì‹ ê·œ] price_workerê°€ ëª¨ì•„ë‘” ìµœì‹  ê°€ê²© ê°€ì ¸ì˜¤ê¸°
def get_current_prices():
    try:
        # price_cache í…Œì´ë¸”ì—ì„œ ì „ì²´ ì¡°íšŒ
        res = supabase.table("price_cache").select("ticker, price").execute()
        return {item['ticker']: float(item['price']) for item in res.data if item['price']}
    except:
        return {}

# ==========================================
# [3] AI ë¶„ì„ í•¨ìˆ˜ë“¤ (ë¹„ìš© ìµœì í™” ì ìš©)
# ==========================================

# Tab 0: ì¼ë°˜ ëª¨ë¸ (ë¬´ë£Œ)
def run_tab0_analysis(ticker, company_name):
    if not standard_model: return
    
    def_meta = {
        "S-1": "Risk Factors(íŠ¹ì´ ì†Œì†¡/ê·œì œ), Use of Proceeds(ìê¸ˆ ìš©ë„ì˜ ê±´ì „ì„±), MD&A(ì„±ì¥ ë™ì¸)",
        "S-1/A": "Pricing Terms(ìˆ˜ìš”ì˜ˆì¸¡ ë¶„ìœ„ê¸°), Dilution(ì‹ ê·œ íˆ¬ìì í¬ì„ë¥ ), Changes(ì´ì „ ì œì¶œë³¸ê³¼ì˜ ì°¨ì´ì )",
        "F-1": "Foreign Risk(ì§€ì •í•™ì  ë¦¬ìŠ¤í¬), Accounting(GAAP ì°¨ì´), ADS(ì£¼ì‹ ì˜ˆíƒ ì¦ì„œ êµ¬ì¡°)",
        "FWP": "Graphics(ì‹œì¥ ì ìœ ìœ¨ ì‹œê°í™”), Strategy(ë¯¸ë˜ í•µì‹¬ ë¨¹ê±°ë¦¬), Highlights(ê²½ì˜ì§„ ê°•ì¡° ì‚¬í•­)",
        "424B4": "Underwriting(ì£¼ê´€ì‚¬ ë“±ê¸‰), Final Price(ê¸°ê´€ ë°°ì • ë¬¼ëŸ‰), IPO Outcome(ìµœì¢… ê³µëª¨ ê²°ê³¼)"
    }

    format_instruction = """
    [ì¶œë ¥ í˜•ì‹ ë° ë²ˆì—­ ê·œì¹™ - ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ]
    - ê° ë¬¸ë‹¨ì˜ ì‹œì‘ì€ ë°˜ë“œì‹œ í•´ë‹¹ ì–¸ì–´ë¡œ ë²ˆì—­ëœ **[ì†Œì œëª©]**ìœ¼ë¡œ ì‹œì‘í•œ ë’¤, ì¤„ë°”ê¿ˆ ì—†ì´ í•œ ì¹¸ ë„ìš°ê³  ë°”ë¡œ ë‚´ìš©ì„ ì´ì–´ê°€ì„¸ìš”.
    - [ë¶„ëŸ‰ ì¡°ê±´] ì „ì²´ ìš”ì•½ì´ ì•„ë‹™ë‹ˆë‹¤! **ê° ë¬¸ë‹¨(1, 2, 3)ë§ˆë‹¤ ë°˜ë“œì‹œ 4~5ë¬¸ì¥(ì•½ 5ì¤„ ë¶„ëŸ‰)ì”©** ë‚´ìš©ì„ ìƒì„¸í•˜ê³  í’ì„±í•˜ê²Œ ì±„ì›Œ ë„£ìœ¼ì„¸ìš”.
    - ê¸ˆì§€ ì˜ˆì‹œ: **[Heading - í•œêµ­ì–´]** (X), **[Heading]** \n Content (X)
    """

    for topic in ["S-1", "S-1/A", "F-1", "FWP", "424B4"]:
        if topic not in def_meta: continue
        points = def_meta[topic]
        
        for lang_code, target_lang in SUPPORTED_LANGS.items():
            cache_key = f"{company_name}_{topic}_Tab0_v11_{lang_code}"
            
            prompt = f"""
            Role: Wall Street Senior Analyst.
            Task: Analyze {company_name} ({ticker})'s {topic} filing points: {points}.
            Language: Strictly in {target_lang}.
            
            [Structure]
            1. First paragraph: Analysis of key investment points in the document.
            2. Second paragraph: Analysis of growth potential and financial implications.
            3. Third paragraph: One key risk factor and its impact.

            {format_instruction}
            """
            try:
                response = standard_model.generate_content(prompt)
                batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": response.text, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
                time.sleep(0.5)
            except: pass

# Tab 1: ê²€ìƒ‰ 1íšŒ -> ë²ˆì—­ 3íšŒ (ë¹„ìš© ì ˆê°)
def run_tab1_analysis(ticker, company_name):
    if not search_model or not standard_model: return
    
    # 1. [ê²€ìƒ‰ ë‹¨ê³„] ì˜ì–´ë¡œ 1ë²ˆë§Œ ê²€ìƒ‰
    source_text = ""
    try:
        search_prompt = f"""
        Find the detailed business model and 5 recent news articles (last 1 year) for {company_name} ({ticker}).
        Output the news in JSON format inside <NEWS_JSON> tags, and business summary as plain text.
        """
        source_resp = search_model.generate_content(search_prompt)
        source_text = source_resp.text
    except: return 

    # 2. [ë²ˆì—­ ë‹¨ê³„] 
    for lang_code, target_lang in SUPPORTED_LANGS.items():
        cache_key = f"{ticker}_Tab1_v2_{lang_code}"
        
        if lang_code == 'ja':
            lang_instruction = "å¿…ãšæ—¥æœ¬èª(Japanese)ã®ã¿ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            json_format = f"""{{ "news": [ {{ "title_en": "Original Title", "translated_title": "æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«", "link": "...", "sentiment": "ê¸ì •/ë¶€ì •/ì¼ë°˜", "date": "YYYY-MM-DD" }} ] }}"""
        elif lang_code == 'en':
            lang_instruction = "Write strictly in English."
            json_format = f"""{{ "news": [ {{ "title_en": "Original Title", "translated_title": "Original Title", "link": "...", "sentiment": "ê¸ì •/ë¶€ì •/ì¼ë°˜", "date": "YYYY-MM-DD" }} ] }}"""
        else:
            lang_instruction = "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."
            json_format = f"""{{ "news": [ {{ "title_en": "Original Title", "translated_title": "í•œêµ­ì–´ ì œëª©", "link": "...", "sentiment": "ê¸ì •/ë¶€ì •/ì¼ë°˜", "date": "YYYY-MM-DD" }} ] }}"""

        prompt = f"""
        Based on the provided source info below, create a report for {company_name} ({ticker}).
        Source Info: {source_text[:10000]} 

        [Task 1: Business Model]
        - Write 3 paragraphs (Model, Financials, Outlook) in {target_lang}. {lang_instruction}
        - No headers, just plain text paragraphs.

        [Task 2: News]
        - Extract 5 news from source and format as JSON.
        - Important: Keep 'sentiment' value as "ê¸ì •", "ë¶€ì •", or "ì¼ë°˜" (Korean) regardless of output language.
        
        <JSON_START>
        {json_format}
        <JSON_END>
        """
        try:
            response = standard_model.generate_content(prompt)
            full_text = response.text
            
            biz_analysis = full_text.split("<JSON_START>")[0].strip()
            biz_analysis = re.sub(r'#.*', '', biz_analysis).strip()
            paragraphs = [p.strip() for p in biz_analysis.split('\n') if len(p.strip()) > 20]
            
            indent_size = "14px" if lang_code == "ko" else "0px"
            html_output = "".join([f'<p style="display:block; text-indent:{indent_size}; margin-bottom:20px; line-height:1.8; text-align:justify; font-size: 15px; color: #333;">{p}</p>' for p in paragraphs])
            
            news_list = []
            if "<JSON_START>" in full_text:
                try: 
                    json_part = full_text.split("<JSON_START>")[1].split("<JSON_END>")[0].strip()
                    news_list = json.loads(json_part).get("news", [])
                except: pass
            
            batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": json.dumps({"html": html_output, "news": news_list}, ensure_ascii=False), "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
            time.sleep(1)
        except: pass

# Tab 3: ì¼ë°˜ ëª¨ë¸ (ë¬´ë£Œ)
def run_tab3_analysis(ticker, company_name, metrics):
    if not standard_model: return
    for lang_code, target_lang in SUPPORTED_LANGS.items():
        cache_key = f"{ticker}_Financial_Report_Tab3_{lang_code}"
        prompt = f"""
        Role: CFA Analyst.
        Task: Write a financial report for {company_name} based on: {metrics}.
        Language: {target_lang}.
        Format: 4 sections [Valuation], [Operating], [Risk], [Conclusion].
        Length: 10-12 lines total.
        """
        try:
            response = standard_model.generate_content(prompt)
            batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": response.text, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
            time.sleep(0.5)
        except: pass

# Tab 4: ê²€ìƒ‰ 1íšŒ -> ë²ˆì—­ 3íšŒ (ë¹„ìš© ì ˆê°)
def run_tab4_analysis(ticker, company_name):
    if not search_model or not standard_model: return

    # 1. [ê²€ìƒ‰ ë‹¨ê³„] ì˜ì–´ë¡œ ê¸°ê´€ ë¦¬í¬íŠ¸ ê²€ìƒ‰
    source_text = ""
    try:
        search_prompt = f"Find recent institutional analyst ratings, price targets, and pros/cons reports for {company_name} ({ticker})."
        source_resp = search_model.generate_content(search_prompt)
        source_text = source_resp.text
    except: return

    # 2. [ë²ˆì—­ ë‹¨ê³„]
    for lang_code, target_lang in SUPPORTED_LANGS.items():
        cache_key = f"{ticker}_Tab4_{lang_code}"
        
        if lang_code == 'ja':
            json_format = '"summary": "3è¡Œè¦ç´„", "pro_con": "**Pros(é•·æ‰€)**:\\n- å†…å®¹\\n\\n**Cons(çŸ­æ‰€)**:\\n- å†…å®¹ (å¿…ãšæ—¥æœ¬èªã§)",'
        elif lang_code == 'en':
            json_format = '"summary": "3-line summary", "pro_con": "**Pros**:... **Cons**:..."'
        else:
            json_format = '"summary": "3ì¤„ ìš”ì•½", "pro_con": "**Pros(ì¥ì )**... **Cons(ë‹¨ì )**..."'

        prompt = f"""
        Using the source info below, create an institutional report summary for {company_name} ({ticker}).
        Source Info: {source_text[:8000]}
        Language: {target_lang} (Strictly).
        
        <JSON_START>
        {{
            "rating": "Buy/Hold/Sell",
            {json_format},
            "links": [{{"title": "Report Title", "link": "URL"}}]
        }}
        <JSON_END>
        """
        try:
            response = standard_model.generate_content(prompt)
            match = re.search(r'<JSON_START>(.*?)<JSON_END>', response.text, re.DOTALL)
            if match:
                clean_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', match.group(1).strip())
                batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": clean_str, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
            time.sleep(1)
        except: pass

def update_macro_data(df):
    if not standard_model: return
    print("ğŸŒ ê±°ì‹œ ì§€í‘œ(Tab 2) ì—…ë°ì´íŠ¸ ì¤‘...")
    data = {"ipo_return": 15.2, "ipo_volume": len(df), "vix": 14.5, "fear_greed": 60} 
    
    for lang_code, target_lang in SUPPORTED_LANGS.items():
        cache_key = f"Global_Market_Dashboard_Tab2_{lang_code}"
        try:
            prompt = f"Market Data: {data}. Write a 3-line daily market briefing in {target_lang}. No headers."
            ai_resp = standard_model.generate_content(prompt).text
            ai_resp = re.sub(r'^#+.*$', '', ai_resp, flags=re.MULTILINE).strip()
            batch_upsert("analysis_cache", [{"cache_key": cache_key, "content": ai_resp, "updated_at": datetime.now().isoformat()}], on_conflict="cache_key")
        except: pass

# ==========================================
# [4] ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ==========================================
def main():
    print(f"ğŸš€ Worker Start: {datetime.now()}")
    
    df = get_target_stocks()
    if df.empty: 
        print("âš ï¸ ìˆ˜ì§‘ëœ IPO ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # [1] ì „ì²´ ëª…ë‹¨ DB ì—…ë°ì´íŠ¸ (Hot ì—¬ë¶€ ìƒê´€ì—†ì´ ëª©ë¡ì€ ìµœì‹ í™”)
    print("\nğŸ“‹ [stock_cache] ëª…ë‹¨ ì—…ë°ì´íŠ¸...")
    now_iso = datetime.now().isoformat()
    stock_list = [{"symbol": str(row['symbol']), "name": str(row['name']) or "Unknown", "last_updated": now_iso} for _, row in df.iterrows()]
    batch_upsert("stock_cache", stock_list, on_conflict="symbol")

    # [2] ë§¤í¬ë¡œ ì—…ë°ì´íŠ¸ (ë¹„ìš© ê±°ì˜ ì—†ìŒ)
    update_macro_data(df)
    
    # ----------------------------------------------------
    # ğŸ’¡ [í•µì‹¬] Hot ì¢…ëª© ì„ ë³„ ë¡œì§ (ìƒì¥ ì˜ˆì • + ìƒìœ„ ìˆ˜ìµë¥  30ìœ„)
    # ----------------------------------------------------
    print("ğŸ”¥ Hot ì¢…ëª© ì„ ë³„ ì¤‘...")
    price_map = get_current_prices() # price_workerê°€ ëª¨ì€ ìµœì‹  ê°€ê²©
    
    today = datetime.now()
    hot_symbols = set()
    
    # (1) ìƒì¥ ì˜ˆì • ì¢…ëª© (ì˜¤ëŠ˜ ì´í›„ ~ 35ì¼ ì´ë‚´)
    try:
        df['dt'] = pd.to_datetime(df['date'])
        upcoming = df[(df['dt'] > today) & (df['dt'] <= today + timedelta(days=35))]
        hot_symbols.update(upcoming['symbol'].tolist())
        print(f"   -> ìƒì¥ ì˜ˆì •: {len(upcoming)}ê°œ")
    except: pass
    
    # (2) ìµœê·¼ 12ê°œì›” ìƒì¥ ì¤‘ ìˆ˜ìµë¥  ìƒìœ„ 30ê°œ
    try:
        past_12m = df[(df['dt'] >= today - timedelta(days=365)) & (df['dt'] <= today)].copy()
        
        # ìˆ˜ìµë¥  ê³„ì‚° í•¨ìˆ˜
        def calc_return(row):
            try:
                # IPO ê°€ê²© íŒŒì‹± ($10.00-12.00 í˜•íƒœ ì²˜ë¦¬)
                ipo_p_str = str(row.get('price', '0')).replace('$','').split('-')[0]
                ipo_p = float(ipo_p_str)
                curr_p = price_map.get(row['symbol'], 0.0)
                
                if ipo_p > 0 and curr_p > 0:
                    return (curr_p - ipo_p) / ipo_p * 100
                return -9999.0 # ê°€ê²© ì •ë³´ ì—†ìœ¼ë©´ í•˜ìœ„ë¡œ
            except:
                return -9999.0
        
        past_12m['return'] = past_12m.apply(calc_return, axis=1)
        top_30 = past_12m.sort_values(by='return', ascending=False).head(30)
        hot_symbols.update(top_30['symbol'].tolist())
        print(f"   -> ìˆ˜ìµë¥  ìƒìœ„: 30ê°œ (1ìœ„: {top_30.iloc[0]['symbol']} {top_30.iloc[0]['return']:.1f}%)")
        
    except Exception as e:
        print(f"   âš ï¸ ìˆ˜ìµë¥  ê³„ì‚° ì¤‘ ì—ëŸ¬: {e}")

    print(f"âœ… ìµœì¢… Hot ì¢…ëª©: ì´ {len(hot_symbols)}ê°œ")

    # ----------------------------------------------------
    # [3] ë¶„ì„ ë£¨í”„ ì‹œì‘
    # ----------------------------------------------------
    total = len(df)
    print(f"\nğŸ¤– AI ì‹¬ì¸µ ë¶„ì„ ì‹œì‘ (ì´ {total}ê°œ ì¤‘ Hot ì¢…ëª© ìœ„ì£¼ ì‹¤í–‰)...")
    
    for idx, row in df.iterrows():
        symbol = row.get('symbol')
        name = row.get('name')
        
        is_hot = symbol in hot_symbols
        # ì›”ìš”ì¼ì´ê±°ë‚˜ Hot ì¢…ëª©ì´ë©´ ì „ì²´ ì—…ë°ì´íŠ¸ (ê·¸ ì™¸ì—ëŠ” ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ìŠ¤í‚µ)
        is_full_update = (today.weekday() == 0 or is_hot)
        
        print(f"[{idx+1}/{total}] {symbol} (Hot:{is_hot}) ì²˜ë¦¬ ì¤‘...", flush=True)
        
        try:
            # 1. Tab 1, 4 (ëˆ ë“œëŠ” ê²€ìƒ‰ ëª¨ë¸): ì˜¤ì§ Hot ì¢…ëª©ë§Œ ì‹¤í–‰!
            if is_hot:
                run_tab1_analysis(symbol, name)
                if is_full_update:
                    run_tab4_analysis(symbol, name)
            
            # 2. Tab 0, 3 (ëˆ ì•ˆ ë“œëŠ” ì¼ë°˜ ëª¨ë¸): í•„ìš” ì‹œ ì‹¤í–‰ (ë¹„ìš© ë¶€ë‹´ ì—†ìŒ)
            if is_full_update:
                run_tab0_analysis(symbol, name)
                try:
                    tk = yf.Ticker(symbol)
                    run_tab3_analysis(symbol, name, {"pe": tk.info.get('forwardPE', 0)})
                except: pass
            
            time.sleep(0.5) 
            
        except Exception as e:
            print(f"âš ï¸ {symbol} ê±´ë„ˆëœ€: {e}")
            continue
            
    print(f"\nğŸ ëª¨ë“  ì‘ì—… ì¢…ë£Œ: {datetime.now()}")

if __name__ == "__main__":
    main()
